{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from scipy import interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 66, 260, 400])\n"
     ]
    }
   ],
   "source": [
    "disp_f2m_ = torch.load('./disp_5.pth')\n",
    "print(disp_f2m_.size())\n",
    "# torch.save(disp_f2m_,'disp_5.pth')\n",
    "origin_itk_ = [-250,-201,-158.5]\n",
    "cor_pixel_=[257,207,33]\n",
    "pixel_spacing_ = [0.9766, 0.9766, 5]\n",
    "crop_range_ = [0,170,50]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SpatialTransformer(nn.Module):\n",
    "    # 2D or 3d spatial transformer network to calculate the warped moving image\n",
    "\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.grid_dict = {}\n",
    "        self.norm_coeff_dict = {}\n",
    "\n",
    "    def forward(self, input_image, flow):\n",
    "        '''\n",
    "        input_image: (n, 1, h, w) or (n, 1, d, h, w)\n",
    "        flow: (n, 2, h, w) or (n, 3, d, h, w)\n",
    "\n",
    "        return:\n",
    "            warped moving image, (n, 1, h, w) or (n, 1, d, h, w)\n",
    "        '''\n",
    "        img_shape = input_image.shape[2:]\n",
    "        if img_shape in self.grid_dict:\n",
    "            grid = self.grid_dict[img_shape]\n",
    "            norm_coeff = self.norm_coeff_dict[img_shape]\n",
    "        else:\n",
    "            grids = torch.meshgrid([torch.arange(0, s) for s in img_shape])\n",
    "            grid = torch.stack(grids[::-1],\n",
    "                               dim=0)  # 2 x h x w or 3 x d x h x w, the data in second dimension is in the order of [w, h, d]\n",
    "            grid = torch.unsqueeze(grid, 0)\n",
    "            grid = grid.to(dtype=flow.dtype, device=flow.device)\n",
    "            norm_coeff = 2. / (torch.tensor(img_shape[::-1], dtype=flow.dtype,\n",
    "                                            device=flow.device) - 1.)  # the coefficients to map image coordinates to [-1, 1]\n",
    "            self.grid_dict[img_shape] = grid\n",
    "            self.norm_coeff_dict[img_shape] = norm_coeff\n",
    "            # logging.info(f'\\nAdd grid shape {tuple(img_shape)}')\n",
    "        new_grid = grid + flow\n",
    "\n",
    "        if self.dim == 2:\n",
    "            new_grid = new_grid.permute(0, 2, 3, 1)  # n x h x w x 2\n",
    "        elif self.dim == 3:\n",
    "            new_grid = new_grid.permute(0, 2, 3, 4, 1)  # n x d x h x w x 3\n",
    "\n",
    "        if len(input_image) != len(new_grid):\n",
    "            # make the image shape compatable by broadcasting\n",
    "            input_image += torch.zeros_like(new_grid)\n",
    "            new_grid += torch.zeros_like(input_image)\n",
    "\n",
    "        warped_input_img = F.grid_sample(input_image, new_grid * norm_coeff - 1., mode='bilinear', align_corners=True,\n",
    "                                         padding_mode='border')\n",
    "        return warped_input_img\n",
    "\n",
    "def pixel_to_itk(pixel, origin, spacing):\n",
    "    x = (pixel[0]-1)*spacing[0]+origin[0]\n",
    "    y = (pixel[1]-1)*spacing[1]+origin[1]\n",
    "    z = (pixel[2]-1)*spacing[2]+origin[2]\n",
    "    return [x,y,z]\n",
    "\n",
    "def itk_to_pixel(itk, origin, spacing):\n",
    "    x = (itk[0] -origin[0])/spacing[0]+1\n",
    "    y = (itk[1] -origin[1])/spacing[1]+1\n",
    "    z = (itk[2] -origin[2])/spacing[2]+1\n",
    "    return [x,y,z]\n",
    "\n",
    "def pixel_to_crop_pixel(pixel, crop_range):\n",
    "    x = pixel[0]-crop_range[2]\n",
    "    y = pixel[1]-crop_range[1]\n",
    "    z = pixel[2]-crop_range[0]\n",
    "    return [x,y,z]\n",
    "\n",
    "def inverse_disp(disp, threshold=0.01, max_iteration=20):\n",
    "    '''\n",
    "    compute the inverse field. implementation of \"A simple fixed‚Äêpoint approach to invert a deformation field\"\n",
    "\n",
    "    disp : (n, 2, h, w) or (n, 3, d, h, w) or (2, h, w) or (3, d, h, w)\n",
    "        displacement field\n",
    "    '''\n",
    "    dim=3\n",
    "    spatial_transformer=SpatialTransformer(dim)\n",
    "    forward_disp = disp.detach().cuda()\n",
    "    if disp.ndim < dim+2:\n",
    "        forward_disp = torch.unsqueeze(forward_disp, 0)\n",
    "    backward_disp = torch.zeros_like(forward_disp)\n",
    "    backward_disp_old = backward_disp.clone()\n",
    "    for i in range(max_iteration):\n",
    "        backward_disp = -spatial_transformer(forward_disp, backward_disp)\n",
    "        diff = torch.max(torch.abs(backward_disp - backward_disp_old)).item()\n",
    "        if diff < threshold:\n",
    "            break\n",
    "        backward_disp_old = backward_disp.clone()\n",
    "    if disp.ndim < dim + 2:\n",
    "        backward_disp = torch.squeeze(backward_disp, 0)\n",
    "\n",
    "    return backward_disp\n",
    "\n",
    "def get_dvf(coordinate_itk, origin_itk, pixel_spacing,crop_range, disp_f2m):\n",
    "    coordinate_pixel = itk_to_pixel(coordinate_itk,origin_itk, pixel_spacing)\n",
    "    coordinate_crop_pixel = pixel_to_crop_pixel(coordinate_pixel, crop_range)\n",
    "\n",
    "    disp_m2f = inverse_disp(disp_f2m, threshold=0.01, max_iteration=20)\n",
    "    image_shape = disp_m2f.size()[1:]\n",
    "    grid_tuple = [np.arange(grid_length, dtype=np.float32) for grid_length in image_shape]\n",
    "\n",
    "    inter = interpolate.RegularGridInterpolator(grid_tuple,\n",
    "                                                np.moveaxis(disp_m2f.detach().cpu().numpy(), 0, -1))\n",
    "    pred = inter(np.flip(coordinate_crop_pixel,0))\n",
    "    x = pred[0][0]*pixel_spacing[0]\n",
    "    y = pred[0][1]*pixel_spacing[1]\n",
    "    z = pred[0][2]*pixel_spacing[2]\n",
    "    return [x,y,z]\n",
    "    # return pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[-0.4760943556981456, -1.8668366722960572, 5.167355481570456]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_dvf(coordinate_itk=[74.23,98.82,6.48], origin_itk=origin_itk_,\n",
    "        pixel_spacing=pixel_spacing_,crop_range = crop_range_,\n",
    "        disp_f2m=disp_f2m_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}